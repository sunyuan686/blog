# [走近 LLM：用通俗语言理解 ChatGPT 背后的工作原理（基于 Karpathy「Deep Dive into LLMs」）](https://github.com/sunyuan686/blog/issues/27)


— 致谢：本文严格依据上述视频的逐字转录整理与重述，未引入未经视频验证的信息。[Deep Dive into LLMs like ChatGPT](https://youtu.be/7xTGNNLPyMI?si=76Bd6bP4lkb-7ldT)

TL;DR(Too Long; Didn’t Read)
- LLM 的核心是“预测下一个token”的大型神经网络，通过互联网预训练与对话后训练逐步形成助理能力。
- 预训练学习通用知识，后训练用人类标注的对话数据“教出”助理式行为与拒答规范。
- 幻觉是系统性现象：模型会编造；通过工具使用（搜索、代码执行）与链式思考可显著降低错误。
- “思考型模型”引入强化学习，在可验证任务上能涌现超越人类直觉的策略（类比 AlphaGo 的“第37手”）。
- 正确姿势：把 LLM 当“工具箱中的工具”，要验算、要分步、要用外部工具，不要盲信输出。


当你在聊天框里敲下一个问题、回车，屏幕上出现的答案并不是魔法的低语。它更像是一位受过系统训练的专业“标注者”的统计学模拟：礼貌、 helpful、 truthful、 harmless。它擅长快速生成合理答案，却也会一本正经地胡说八道。理解它如何被训练、为何会出错、怎样让它更可靠，是把 AI 变成生产力而非不稳定因素的关键。

## 1. LLM 是什么：一个“预测下一个词”的函数
- 定义（首次出现内联）：大语言模型（LLM, Large Language Model）是以“预测下一个 token（最小文本单位）”为目标训练的神经网络，可根据上下文生成文本。[
- 神经网络本质：输入 token 序列，经嵌入、注意力、前馈等层的固定数学表达，输出下一个 token 的概率分布；网络本身在推理时是无状态的“前向计算”，记忆来自上下文与参数中凝固的统计模式。
- 为什么重要
  - 明确“预测”目标有助于理解它的优势（语言生成、类比）与局限（精确计算、计数、事实核验）。
- 常见坑
  - 把神经元类比为生物神经元，进而高估“内在意识”与“长期记忆”。LLM 的前向计算是短平快的统计变换。

## 2. 训练一览：从预训练到后训练（Post-training）
### 2.1 预训练：吞下互联网，学习通用模式
- 数据目标：高质量、大规模、多样性互联网文本（如 FineWeb 同类），在数千台机器上训练数月，学习语言与世界的统计结构。
- 你可以把它看作：通过“看到大量文本”习得“语言常识与事实共识”。

### 2.2 后训练（对话对齐）：把“语言模型”调成“助理”
- 定义（内联）：后训练（post-training）指用人工构造的对话数据（多人回合、含人格、包含拒答规则）继续训练，使模型学会“助理式”行为。
- 数据如何来：公司编写详尽“标注说明”（helpful、truthful、harmless），雇佣专业标注者（含领域专家）为多样提示编写“理想回答”。
- 训练方式：在同一算法下短时间续训（可能仅数小时），用对话替换互联网数据，快速对齐模型的输出风格与价值观。
- 为什么重要
  - 解释了“你在和谁对话”：统计上，你在和“受说明约束的人类标注者的平均模拟体”交流。
- 常见坑
  - 想象模型“查过资料后再回答”。实际多为统计拟合，特定事实未必核验，易引出幻觉。

## 3. 推理与采样：答案不是唯一的
- 生成机制：给定前缀，模型输出各 token 概率；通过抽样（带温度、top-k/p 等）得到下一个 token，重复直到结束。
- 多样性来源：高概率 token 更常被采样，但不是唯一；同一问题可能出现多种合理表述。
- 为什么重要
  - 提示工程（prompting）需控制采样策略；**任务需要稳定时用更保守设置，创意写作可提高多样性。**

## 4. LLM“心理学”：幻觉、工具使用与“需要 token 来思考”
### 4.1 幻觉（Hallucination）
- 定义（内联）：幻觉指模型无依据地编造事实、来源或细节。
- 成因：模型在“统计拟合”下补全高似然文本，并不等价于事实核验；特别在不可验证任务、长程逻辑与冷门事实上更易出错。
- 缓解策略：引入工具（检索、代码、计算器）、要求中间步（链式推理、草稿再定稿）、限制任务范围。

### 4.2 工具使用（Tool Use）
- 实践要点：
  - Web/检索：把事实核验外包给搜索/数据库，减少凭记忆回答。
  - 代码解释器：让模型“写程序再求值”，比心算更可靠，适合数学、计数、解析结构化文本等。
- 为什么重要
  - 把复杂任务拆为可验证子任务，显著提升正确率与可审计性。
- 常见坑
  - 让模型一次性在“一个 token”内完成太多计算，导致错算、漏算；应要求逐步展开与外部求值。

### 4.3 “模型需要 token 来思考”
- 直观理解：单次前向计算的“脑回路”有限，要求“一步到位”容易错；通过生成中间结果、分步推理、外部执行，给模型“更多 token 空间”来展开思路。
- 例：计数点位、长算式，直接回答易错；“use code”→ 将输入复制为字符串，用程序 count/计算，得到正确答案。

## 5. 从监督微调到强化学习：超越模仿，走向“思考型模型”
### 5.1 监督微调（SFT）与 RLHF
- 监督微调（内联）：用人类写的理想回答直接训练模型去模仿。
- RLHF（内联）：人类先对多候选回答排序，训练“奖励模型”；再用强化学习优化模型，使其倾向“更被人偏好”的回答。

### 5.2 强化学习与“Move 37”时刻
- 类比 AlphaGo：通过自我博弈与奖励信号，探索超越人类经验分布的策略。出现“第37手”这类低人类概率但事后被证明精彩的着法。
- 对 LLM 的启示：在可验证的推理/数学等任务上，强化学习可能催生新颖推理策略，非单纯“标注者模拟”。
- 开放问题：在不可验证领域（如创意写作）中，“可转移的思考策略”能否泛化，仍在探索中。
- 为什么重要
  - 指向“思考型模型”的方向：不仅模仿答案，更优化“思考过程”。

## 6. 使用建议：把 LLM 当“会写草稿的助理”，而非“不会错的先知”
### 6.1 正确姿势
- 把问题拆小、逐步生成中间结果。
- 优先让模型使用工具（检索、代码执行），对关键结论做外部校验。
- 在可验证领域（编程、数学、数据处理）发挥其优势。

### 6.2 常见坑
- 盲信输出、不核验事实；让模型“心算大题”；一次性要求复杂推理与多跳事实链；忽视后训练价值观带来的“合理但无依据”表达。


行动清单:
- 设计 Prompt 时明确任务边界，要求“先思考后作答”，并允许中间结果输出。
- 对涉及事实/数值的内容，优先让模型使用检索或代码执行，再给出结论。
- 对长问题拆分子问题，逐段求解后再汇总。
- 在编程/数学/数据分析中，以“生成代码+运行校验”为默认流程。
- 对创意性任务，提升采样多样性；对精确任务，收紧采样并强制工具调用。


致谢
- 本文完全基于 Andrej Karpathy 视频“Deep Dive into LLMs like ChatGPT”之转录内容整理与重述，感谢其开放分享。
